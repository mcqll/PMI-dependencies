{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('pmi_accuracy')\n",
    "import txt_to_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Quadro RTX 8000\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Language model 'bert-base-cased' (with batchsize = 32) initialized on cuda.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:',\n",
    "            round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ',\n",
    "            round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')\n",
    "    print()\n",
    "\n",
    "MODEL = txt_to_pmi.languagemodel.BERT(\n",
    "    DEVICE, 'bert-base-cased', 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sentence 0: ['Here', 'is', 'an', 'exemplary', 'sentence.']\n",
      "\n",
      "• Token list:     ['Here', 'is', 'an', 'exemplary', 'sentence.']\n",
      "• Subword tokens: ['Here', 'is', 'an', 'ex', '##em', '##p', '##lary', 'sentence', '.']\n",
      "• tok->span:      [(0,), (1,), (2,), (3, 4, 5, 6), (7, 8)]\n",
      "• span->tok:      {(0,): 0, (1,): 1, (2,): 2, (3, 4, 5, 6): 3, (7, 8): 4}\n",
      "• padleft:        [101]\n",
      "• padright:       [102]\n",
      "• input_ids:      [101, 3446, 1110, 1126, 4252, 5521, 1643, 18480, 5650, 119, 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bert-base-cased: batches:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "bert-base-cased: batches:  50%|█████     | 1/2 [00:01<00:01,  1.82s/it]\u001b[A\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.89s/it]                          \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• pseudo_loglik: -31.351733036339283\n",
      "• cpmi matrix:\n",
      "[[ 0.00  1.05 -0.85  3.62 -0.90]\n",
      " [ 0.54  0.00  0.14  0.08  0.22]\n",
      " [-0.53  0.28  0.00  4.01  0.09]\n",
      " [-0.22 -1.42  0.83  0.00 -1.52]\n",
      " [ 2.45  0.68  0.46  3.81  0.00]]\n",
      "\n",
      "sentence 1: ['This', 'one', 'follows.']\n",
      "\n",
      "• Token list:     ['This', 'one', 'follows.']\n",
      "• Subword tokens: ['This', 'one', 'follows', '.']\n",
      "• tok->span:      [(0,), (1,), (2, 3)]\n",
      "• span->tok:      {(0,): 0, (1,): 1, (2, 3): 2}\n",
      "• padleft:        [101]\n",
      "• padright:       [102]\n",
      "• input_ids:      [101, 1188, 1141, 3226, 119, 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bert-base-cased: batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• pseudo_loglik: -14.565191261470318\n",
      "• cpmi matrix:\n",
      "[[ 0.00  1.78  2.54]\n",
      " [-1.32  0.00 -0.90]\n",
      " [-0.01 -1.22  0.00]]\n",
      "\n",
      "----------\n",
      "CPMI matrix value at position [i,j] is\n",
      " pmi(w_i, w_j | c) = log p(w_i | c) - log p(w_i | c without w_j)\n",
      " where w_i is ith word in sentence, and c is rest of sentence.\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "file = [\"Here is an exemplary sentence.\", \"This one follows.\"]\n",
    "sentences = [line.strip().split(' ')\n",
    "                     for line in file]\n",
    "\n",
    "outs = txt_to_pmi.get_cpmi(MODEL, sentences, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (cpmi)",
   "language": "python",
   "name": "cpmi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
