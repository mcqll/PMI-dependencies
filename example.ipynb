{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('pmi_accuracy')\n",
    "import txt_to_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Quadro RTX 8000\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Language model 'bert-base-cased' (with batchsize = 32) initialized on cuda.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:',\n",
    "            round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ',\n",
    "            round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')\n",
    "    print()\n",
    "\n",
    "MODEL = txt_to_pmi.languagemodel.BERT(\n",
    "    DEVICE, 'bert-base-cased', 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sentence 0: ['Solo', 'woodwind', 'players', 'have', 'to', 'be', 'creative', 'if', 'they', 'want', 'to', 'work', 'a', 'lot']\n",
      "\n",
      "• Token list:     ['Solo', 'woodwind', 'players', 'have', 'to', 'be', 'creative', 'if', 'they', 'want', 'to', 'work', 'a', 'lot']\n",
      "• Subword tokens: ['Solo', 'wood', '##wind', 'players', 'have', 'to', 'be', 'creative', 'if', 'they', 'want', 'to', 'work', 'a', 'lot']\n",
      "• tok->span:      [(0,), (1, 2), (3,), (4,), (5,), (6,), (7,), (8,), (9,), (10,), (11,), (12,), (13,), (14,)]\n",
      "• span->tok:      {(0,): 0, (1, 2): 1, (3,): 2, (4,): 3, (5,): 4, (6,): 5, (7,): 6, (8,): 7, (9,): 8, (10,): 9, (11,): 10, (12,): 11, (13,): 12, (14,): 13}\n",
      "• padleft:        [101]\n",
      "• padright:       [102]\n",
      "• input_ids:      [101, 11977, 3591, 11129, 2139, 1138, 1106, 1129, 6228, 1191, 1152, 1328, 1106, 1250, 170, 1974, 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bert-base-cased: batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "bert-base-cased: batches:  86%|████████▌ | 6/7 [00:00<00:00, 51.76it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:00<00:00,  6.64it/s]                          \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• pseudo_loglik: -39.27373409352731\n",
      "• cpmi matrix:\n",
      "[[ 0.00e+00  3.24e+00  1.79e+00 -3.16e-01 -1.35e-01 -5.19e-02  5.06e-01\n",
      "   1.25e+00  1.32e-01 -2.27e-01  7.98e-02  9.18e-02  1.65e-01  3.31e-01]\n",
      " [ 1.27e+00  0.00e+00  8.03e+00  1.11e+00 -6.23e-01  1.94e-01  5.65e-01\n",
      "  -1.12e-01 -4.94e-01 -5.49e-01 -3.01e-01  8.03e-01 -1.36e-01 -5.24e-01]\n",
      " [-3.51e-01  6.36e+00  0.00e+00 -3.53e-02  7.17e-02 -1.43e-01  5.33e-01\n",
      "   8.30e-02  7.15e-01  8.18e-02  4.06e-02  1.19e-01  4.45e-02  1.06e-01]\n",
      " [-9.15e-02  1.21e-01 -7.79e-01  0.00e+00  1.41e+00  4.30e-01  1.00e-01\n",
      "   4.35e-01  1.69e-01  1.04e-01 -1.02e-01 -2.50e-01 -1.76e-01 -1.83e-01]\n",
      " [ 1.02e-03  2.62e-04 -9.70e-05  8.12e-01  0.00e+00  1.78e+00  1.27e-04\n",
      "   6.79e-04  1.79e-04 -5.24e-06  1.25e-05  9.34e-05  4.91e-05  8.71e-05]\n",
      " [-5.05e-03  8.58e-02 -3.92e-02  9.71e-02  4.64e+00  0.00e+00  2.63e+00\n",
      "  -1.58e-02 -5.85e-03 -1.60e-02  3.57e-03  4.48e-02 -3.52e-03 -4.28e-02]\n",
      " [-6.38e-02  4.49e-01  5.64e-01  4.73e-02  2.76e-01  4.48e+00  0.00e+00\n",
      "  -1.34e+00  4.03e-01 -1.07e-02  6.54e-02 -2.98e-01  3.18e-01  1.59e+00]\n",
      " [ 9.31e-01  8.30e-02  1.98e-01  1.05e+00  2.38e-01 -2.04e-01 -2.88e-01\n",
      "   0.00e+00  3.82e+00  1.93e+00  6.61e-02  3.40e-01 -1.70e-01 -3.73e-01]\n",
      " [ 1.24e-02  5.39e-03  4.52e-01  4.12e-03  6.40e-03 -1.44e-03  3.13e-03\n",
      "   1.43e-01  0.00e+00  1.04e-02  3.33e-03  1.61e-02  9.24e-03  5.07e-03]\n",
      " [ 4.42e-02  9.06e-02  3.63e-01  1.12e-01  1.05e-01 -6.70e-03  4.14e-01\n",
      "   2.20e+00  2.94e-01  0.00e+00 -5.45e-03 -1.11e-01 -1.89e-01 -2.32e-01]\n",
      " [-4.99e-04  2.63e-03 -2.40e-03 -6.64e-05 -1.23e-03  1.25e-03 -1.77e-04\n",
      "  -1.95e-03  6.18e-03  7.60e-02  0.00e+00  2.85e-01  4.70e-04  3.79e-03]\n",
      " [-1.04e-01  3.36e-01 -2.56e-02 -1.69e-01 -1.42e-01 -8.97e-02  2.03e-01\n",
      "  -1.68e-02  1.30e-01 -2.44e-01 -1.90e-01  0.00e+00 -1.19e-01  2.35e+00]\n",
      " [ 4.73e-03  2.98e-02  2.26e-02 -3.70e-03 -6.98e-03  1.56e-03  2.51e-02\n",
      "  -5.95e-03  2.21e-03  6.04e-03 -5.77e-03 -5.54e-03  0.00e+00  1.04e+01]\n",
      " [-9.36e-01 -5.15e-02  4.82e-01 -2.27e-01 -3.08e-01  3.36e-01  5.74e-01\n",
      "  -5.49e-01 -6.13e-02  3.18e-01  6.00e-01  2.29e+00  1.30e+01  0.00e+00]]\n",
      "\n",
      "sentence 1: ['The', 'oboist', 'Heinz', 'Holliger', 'has', 'taken', 'a', 'hard', 'line', 'about', 'the', 'problem:', 'He', 'commissions', 'and', 'splendidly', 'interprets', 'fearsome', 'contemporary', 'scores', 'and', 'does', 'some', 'conducting,', 'so', 'he', \"doesn't\", 'have', 'to', 'play', 'the', 'same', 'Mozart', 'and', 'Strauss', 'concertos', 'over', 'and', 'over', 'again.']\n",
      "\n",
      "• Token list:     ['The', 'oboist', 'Heinz', 'Holliger', 'has', 'taken', 'a', 'hard', 'line', 'about', 'the', 'problem:', 'He', 'commissions', 'and', 'splendidly', 'interprets', 'fearsome', 'contemporary', 'scores', 'and', 'does', 'some', 'conducting,', 'so', 'he', \"doesn't\", 'have', 'to', 'play', 'the', 'same', 'Mozart', 'and', 'Strauss', 'concertos', 'over', 'and', 'over', 'again.']\n",
      "• Subword tokens: ['The', 'o', '##bo', '##ist', 'Heinz', 'Ho', '##lli', '##ger', 'has', 'taken', 'a', 'hard', 'line', 'about', 'the', 'problem', ':', 'He', 'commissions', 'and', 'splendid', '##ly', 'interpret', '##s', 'fears', '##ome', 'contemporary', 'scores', 'and', 'does', 'some', 'conducting', ',', 'so', 'he', 'doesn', \"'\", 't', 'have', 'to', 'play', 'the', 'same', 'Mozart', 'and', 'Strauss', 'concerto', '##s', 'over', 'and', 'over', 'again', '.']\n",
      "• tok->span:      [(0,), (1, 2, 3), (4,), (5, 6, 7), (8,), (9,), (10,), (11,), (12,), (13,), (14,), (15, 16), (17,), (18,), (19,), (20, 21), (22, 23), (24, 25), (26,), (27,), (28,), (29,), (30,), (31, 32), (33,), (34,), (35, 36, 37), (38,), (39,), (40,), (41,), (42,), (43,), (44,), (45,), (46, 47), (48,), (49,), (50,), (51, 52)]\n",
      "• span->tok:      {(0,): 0, (1, 2, 3): 1, (4,): 2, (5, 6, 7): 3, (8,): 4, (9,): 5, (10,): 6, (11,): 7, (12,): 8, (13,): 9, (14,): 10, (15, 16): 11, (17,): 12, (18,): 13, (19,): 14, (20, 21): 15, (22, 23): 16, (24, 25): 17, (26,): 18, (27,): 19, (28,): 20, (29,): 21, (30,): 22, (31, 32): 23, (33,): 24, (34,): 25, (35, 36, 37): 26, (38,): 27, (39,): 28, (40,): 29, (41,): 30, (42,): 31, (43,): 32, (44,): 33, (45,): 34, (46, 47): 35, (48,): 36, (49,): 37, (50,): 38, (51, 52): 39}\n",
      "• padleft:        [101]\n",
      "• padright:       [102]\n",
      "• input_ids:      [101, 1109, 184, 4043, 1776, 18928, 9800, 6473, 2895, 1144, 1678, 170, 1662, 1413, 1164, 1103, 2463, 131, 1124, 15239, 1105, 27120, 1193, 19348, 1116, 10434, 6758, 3793, 7432, 1105, 1674, 1199, 9239, 117, 1177, 1119, 2144, 112, 189, 1138, 1106, 1505, 1103, 1269, 13496, 1105, 17448, 25657, 1116, 1166, 1105, 1166, 1254, 119, 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bert-base-cased: batches:   0%|          | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "bert-base-cased: batches:   4%|▍         | 3/67 [00:00<00:02, 23.00it/s]\u001b[A\n",
      "bert-base-cased: batches:   9%|▉         | 6/67 [00:00<00:02, 23.42it/s]\u001b[A\n",
      "bert-base-cased: batches:  13%|█▎        | 9/67 [00:00<00:02, 24.09it/s]\u001b[A\n",
      "bert-base-cased: batches:  18%|█▊        | 12/67 [00:00<00:02, 24.32it/s]\u001b[A\n",
      "bert-base-cased: batches:  22%|██▏       | 15/67 [00:00<00:02, 24.42it/s]\u001b[A\n",
      "bert-base-cased: batches:  27%|██▋       | 18/67 [00:00<00:01, 24.58it/s]\u001b[A\n",
      "bert-base-cased: batches:  31%|███▏      | 21/67 [00:00<00:01, 24.62it/s]\u001b[A\n",
      "bert-base-cased: batches:  36%|███▌      | 24/67 [00:00<00:01, 24.44it/s]\u001b[A\n",
      "bert-base-cased: batches:  40%|████      | 27/67 [00:01<00:01, 24.16it/s]\u001b[A\n",
      "bert-base-cased: batches:  45%|████▍     | 30/67 [00:01<00:01, 24.13it/s]\u001b[A\n",
      "bert-base-cased: batches:  49%|████▉     | 33/67 [00:01<00:01, 23.99it/s]\u001b[A\n",
      "bert-base-cased: batches:  54%|█████▎    | 36/67 [00:01<00:01, 23.89it/s]\u001b[A\n",
      "bert-base-cased: batches:  58%|█████▊    | 39/67 [00:01<00:01, 23.79it/s]\u001b[A\n",
      "bert-base-cased: batches:  63%|██████▎   | 42/67 [00:01<00:01, 23.81it/s]\u001b[A\n",
      "bert-base-cased: batches:  67%|██████▋   | 45/67 [00:01<00:00, 23.84it/s]\u001b[A\n",
      "bert-base-cased: batches:  72%|███████▏  | 48/67 [00:01<00:00, 23.74it/s]\u001b[A\n",
      "bert-base-cased: batches:  76%|███████▌  | 51/67 [00:02<00:00, 23.71it/s]\u001b[A\n",
      "bert-base-cased: batches:  81%|████████  | 54/67 [00:02<00:00, 23.73it/s]\u001b[A\n",
      "bert-base-cased: batches:  85%|████████▌ | 57/67 [00:02<00:00, 23.81it/s]\u001b[A\n",
      "bert-base-cased: batches:  90%|████████▉ | 60/67 [00:02<00:00, 23.70it/s]\u001b[A\n",
      "bert-base-cased: batches:  94%|█████████▍| 63/67 [00:02<00:00, 23.72it/s]\u001b[A\n",
      "bert-base-cased: batches:  99%|█████████▊| 66/67 [00:02<00:00, 23.68it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.71s/it]                            \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• pseudo_loglik: -146.44325935254756\n",
      "• cpmi matrix:\n",
      "[[ 0.00e+00  5.97e-01  6.41e-03 ...  3.72e-02  2.45e-02  8.16e-02]\n",
      " [ 7.36e+00  0.00e+00 -2.47e+00 ...  1.02e-01  1.61e-01  3.35e-01]\n",
      " [ 9.62e-01  8.14e-01  0.00e+00 ...  1.29e-01  9.40e-02  1.62e-01]\n",
      " ...\n",
      " [ 7.86e-06  3.81e-06 -1.20e-05 ...  0.00e+00  3.50e+00  1.63e-03]\n",
      " [-9.65e-06 -2.95e-05 -3.11e-05 ...  7.17e+00  0.00e+00  1.03e-03]\n",
      " [ 1.56e-03  7.56e-04  1.80e-03 ...  8.24e-01  8.90e+00  0.00e+00]]\n",
      "\n",
      "sentence 2: ['Richard', 'Stoltzman', 'has', 'taken', 'a', 'gentler,', 'more', 'audience-friendly', 'approach.']\n",
      "\n",
      "• Token list:     ['Richard', 'Stoltzman', 'has', 'taken', 'a', 'gentler,', 'more', 'audience-friendly', 'approach.']\n",
      "• Subword tokens: ['Richard', 'St', '##olt', '##zman', 'has', 'taken', 'a', 'gentle', '##r', ',', 'more', 'audience', '-', 'friendly', 'approach', '.']\n",
      "• tok->span:      [(0,), (1, 2, 3), (4,), (5,), (6,), (7, 8, 9), (10,), (11, 12, 13), (14, 15)]\n",
      "• span->tok:      {(0,): 0, (1, 2, 3): 1, (4,): 2, (5,): 3, (6,): 4, (7, 8, 9): 5, (10,): 6, (11, 12, 13): 7, (14, 15): 8}\n",
      "• padleft:        [101]\n",
      "• padright:       [102]\n",
      "• input_ids:      [101, 2055, 1457, 17772, 27277, 1144, 1678, 170, 6892, 1197, 117, 1167, 3703, 118, 4931, 3136, 119, 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bert-base-cased: batches:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "                                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• pseudo_loglik: -42.45163962407969\n",
      "• cpmi matrix:\n",
      "[[ 0.00e+00  3.32e+00  8.61e-02 -7.48e-02  3.04e-02  2.95e-02 -3.34e-02\n",
      "  -2.34e-01  2.71e-01]\n",
      " [ 4.29e+00  0.00e+00  5.02e+00  2.98e-01  1.46e-02  5.82e-01 -3.03e-02\n",
      "   4.69e-01  5.49e-01]\n",
      " [-9.37e-02  2.57e+00  0.00e+00  3.45e+00 -1.37e-01 -1.80e-01 -4.54e-02\n",
      "   7.90e-02  2.48e-01]\n",
      " [-9.41e-02  8.65e-02  5.60e+00  0.00e+00  6.60e+00 -3.27e-01  1.74e-01\n",
      "  -7.13e-01  1.59e+00]\n",
      " [ 9.93e-04  1.54e-03  5.30e-03  2.50e-03  0.00e+00  2.82e+00  6.66e-04\n",
      "   2.80e-03  6.23e-02]\n",
      " [ 2.36e-01  7.28e-01  5.82e-02 -6.69e-02  6.59e+00  0.00e+00  1.26e-01\n",
      "   9.58e-03 -1.04e+00]\n",
      " [-1.18e-02  7.40e-03  1.65e-03 -5.49e-03  2.07e-02  5.00e+00  0.00e+00\n",
      "   9.42e-01 -8.80e-03]\n",
      " [-3.89e-01 -6.84e-02  9.72e-02  3.53e-02 -4.12e-01  7.64e-01  2.89e+00\n",
      "   0.00e+00  7.77e-01]\n",
      " [ 9.76e-03  8.57e-02  1.63e-01  2.00e+00 -2.74e-03  3.94e+00  1.15e-03\n",
      "   7.23e-02  0.00e+00]]\n",
      "\n",
      "----------\n",
      "CPMI matrix value at position [i,j] is\n",
      " pmi(w_i, w_j | c) = log p(w_i | c) - log p(w_i | c without w_j)\n",
      " where w_i is ith word in sentence, and c is rest of sentence.\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "example_file = [\n",
    "    \"Solo woodwind players have to be creative if they want to work a lot\",\n",
    "    \"The oboist Heinz Holliger has taken a hard line about the problem: He commissions and splendidly interprets fearsome contemporary scores and does some conducting, so he doesn't have to play the same Mozart and Strauss concertos over and over again.\", \n",
    "    \"Richard Stoltzman has taken a gentler, more audience-friendly approach.\"]\n",
    "sentences = [line.strip().split(' ') for line in example_file]\n",
    "\n",
    "outs = txt_to_pmi.get_cpmi(MODEL, sentences, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (cpmi)",
   "language": "python",
   "name": "cpmi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
